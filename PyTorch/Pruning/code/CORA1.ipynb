{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "86a9e473",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.datasets import Planetoid\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv,ChebConv, SAGEConv, GATConv, GINConv\n",
    "from torch.nn import Linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e7779738",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Planetoid(root='/tmp/Cora', name='Cora')\n",
    "data = dataset[0] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "af030beb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self, hidden_channels, activationGNN, activationOut):\n",
    "        super(GCN, self).__init__()\n",
    "        torch.manual_seed(42)\n",
    "        \n",
    "        # Initialize the layers\n",
    "        self.conv1 = GCNConv(dataset.num_features, hidden_channels)\n",
    "        self.conv2 = GCNConv(hidden_channels, hidden_channels)\n",
    "        self.fullyConnected = Linear(hidden_channels, dataset.num_classes)\n",
    "        self.activationGNN = activationGNN\n",
    "        self.activationOut = activationOut\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = self.activationGNN(x)\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "\n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = self.activationGNN(x)\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "\n",
    "        if self.activationOut == torch.softmax:\n",
    "            x = self.activationOut(self.fullyConnected(x), dim=1)\n",
    "        else:\n",
    "            x = self.activationOut(self.fullyConnected(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8a2a3648",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.utils.prune as prune"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b640db97",
   "metadata": {},
   "source": [
    "Weights are randomly initialized when creating the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b8c99b5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GCN(hidden_channels=64, activationGNN=F.leaky_relu, activationOut=torch.softmax)\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "criterion = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c8afd062",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Access the weights of each layer\n",
    "conv1_weights = model.conv1.lin.weight.clone()\n",
    "conv2_weights = model.conv2.lin.weight.clone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "466c9d18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0517, grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv2_weights[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9018f60f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "      model.train()\n",
    "      optimizer.zero_grad() \n",
    "      out = model(data.x, data.edge_index)  \n",
    "      # Używamy tylko nodów z labelkami do obliczenia funkcji straty\n",
    "      loss = criterion(out[data.train_mask], data.y[data.train_mask])  \n",
    "      loss.backward() \n",
    "      optimizer.step()\n",
    "      return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "569c32be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test():\n",
    "      model.eval()\n",
    "      out = model(data.x, data.edge_index)\n",
    "      pred = out.argmax(dim=1)  \n",
    "      test_correct = pred[data.test_mask] == data.y[data.test_mask]  \n",
    "      test_acc = int(test_correct.sum()) / int(data.test_mask.sum())  \n",
    "      return test_acc, pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5bc25563",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funckja do trenowania modelu przez zadaną liczbę epok\n",
    "def final_train(epochs):\n",
    "    losses = []\n",
    "    for epoch in range(0, epochs):\n",
    "        loss = train()\n",
    "        losses.append(loss)\n",
    "    return losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fcbfc5a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global sparsity: 0.00%\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    \"Global sparsity: {:.2f}%\".format(\n",
    "        100. * float(\n",
    "            torch.sum(model.conv1.lin.weight == 0)\n",
    "            + torch.sum(model.conv2.lin.weight == 0)\n",
    "        )\n",
    "        / float(\n",
    "            model.conv1.lin.weight.nelement()\n",
    "            + model.conv2.lin.weight.nelement()\n",
    "        )\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7f1d3185",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = final_train(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e2c9daf7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.785, tensor([3, 4, 4,  ..., 5, 3, 3]))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e8b6a55",
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters_to_prune = (\n",
    "    (model.conv1.lin, 'weight'),  # Prune the weights of layer1\n",
    "    (model.conv2.lin, 'weight'),  # Prune the weights of layer2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8131c39",
   "metadata": {},
   "outputs": [],
   "source": [
    "prune.global_unstructured(\n",
    "    parameters_to_prune,\n",
    "    pruning_method=prune.L1Unstructured,  # Use L1-norm based pruning\n",
    "    amount=0.2,  # Prune 20% of the parameters\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1724cbcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global sparsity: 20.00%\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    \"Global sparsity: {:.2f}%\".format(\n",
    "        100. * float(\n",
    "            torch.sum(model.conv1.lin.weight == 0)\n",
    "            + torch.sum(model.conv2.lin.weight == 0)\n",
    "        )\n",
    "        / float(\n",
    "            model.conv1.lin.weight.nelement()\n",
    "            + model.conv2.lin.weight.nelement()\n",
    "        )\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a83cb5e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.779, tensor([3, 4, 4,  ..., 5, 3, 3]))"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = final_train(1000)\n",
    "test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "52cfd0a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters_to_prune = (\n",
    "    (model.conv1.lin, 'weight'),  # Prune the weights of layer1\n",
    "    (model.conv2.lin, 'weight'),  # Prune the weights of layer2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "38ddfb76",
   "metadata": {},
   "outputs": [],
   "source": [
    "prune.global_unstructured(\n",
    "    parameters_to_prune,\n",
    "    pruning_method=prune.L1Unstructured,  # Use L1-norm based pruning\n",
    "    amount=0.4,  # Prune 20% of the parameters\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b7d9fd25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global sparsity: 52.00%\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    \"Global sparsity: {:.2f}%\".format(\n",
    "        100. * float(\n",
    "            torch.sum(model.conv1.lin.weight == 0)\n",
    "            + torch.sum(model.conv2.lin.weight == 0)\n",
    "        )\n",
    "        / float(\n",
    "            model.conv1.lin.weight.nelement()\n",
    "            + model.conv2.lin.weight.nelement()\n",
    "        )\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8709d05a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.777, tensor([3, 4, 4,  ..., 5, 3, 3]))"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = final_train(1000)\n",
    "test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a8e649f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters_to_prune = (\n",
    "    (model.conv1.lin, 'weight'),  # Prune the weights of layer1\n",
    "    (model.conv2.lin, 'weight'),  # Prune the weights of layer2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "763f1578",
   "metadata": {},
   "outputs": [],
   "source": [
    "prune.global_unstructured(\n",
    "    parameters_to_prune,\n",
    "    pruning_method=prune.L1Unstructured,  # Use L1-norm based pruning\n",
    "    amount=0.6,  # Prune 20% of the parameters\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "4fb3bf0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global sparsity: 80.80%\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    \"Global sparsity: {:.2f}%\".format(\n",
    "        100. * float(\n",
    "            torch.sum(model.conv1.lin.weight == 0)\n",
    "            + torch.sum(model.conv2.lin.weight == 0)\n",
    "        )\n",
    "        / float(\n",
    "            model.conv1.lin.weight.nelement()\n",
    "            + model.conv2.lin.weight.nelement()\n",
    "        )\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e5102449",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.791, tensor([3, 4, 4,  ..., 1, 3, 3]))"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = final_train(1000)\n",
    "test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "fe5db025",
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters_to_prune = (\n",
    "    (model.conv1.lin, 'weight'),  # Prune the weights of layer1\n",
    "    (model.conv2.lin, 'weight'),  # Prune the weights of layer2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "4260288b",
   "metadata": {},
   "outputs": [],
   "source": [
    "prune.global_unstructured(\n",
    "    parameters_to_prune,\n",
    "    pruning_method=prune.L1Unstructured,  # Use L1-norm based pruning\n",
    "    amount=0.8,  # Prune 20% of the parameters\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "8799f3e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global sparsity: 92.32%\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    \"Global sparsity: {:.2f}%\".format(\n",
    "        100. * float(\n",
    "            torch.sum(model.conv1.lin.weight == 0)\n",
    "            + torch.sum(model.conv2.lin.weight == 0)\n",
    "        )\n",
    "        / float(\n",
    "            model.conv1.lin.weight.nelement()\n",
    "            + model.conv2.lin.weight.nelement()\n",
    "        )\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "767492c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.78, tensor([3, 4, 4,  ..., 1, 3, 3]))"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = final_train(1000)\n",
    "test()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03bd68f3",
   "metadata": {},
   "source": [
    "<font size=\"5\">Winning tickety </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5f36ea70",
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters_to_prune = (\n",
    "    (model.conv1.lin, 'weight'),  # Prune the weights of layer1\n",
    "    (model.conv2.lin, 'weight'),  # Prune the weights of layer2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0d55989d",
   "metadata": {},
   "outputs": [],
   "source": [
    "prune.global_unstructured(\n",
    "    parameters_to_prune,\n",
    "    pruning_method=prune.L1Unstructured,  # Use L1-norm based pruning\n",
    "    amount=0.2,  # Prune 20% of the parameters\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "17547bba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global sparsity: 20.00%\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    \"Global sparsity: {:.2f}%\".format(\n",
    "        100. * float(\n",
    "            torch.sum(model.conv1.lin.weight == 0)\n",
    "            + torch.sum(model.conv2.lin.weight == 0)\n",
    "        )\n",
    "        / float(\n",
    "            model.conv1.lin.weight.nelement()\n",
    "            + model.conv2.lin.weight.nelement()\n",
    "        )\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98a4c190",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recoverOriginalWeightsW1():\n",
    "    clone = model.conv1.lin.weight.clone()\n",
    "    for i, neuron in enumerate(model.conv1.lin.weight):\n",
    "        for j, elem in enumerate(neuron) :\n",
    "#             print(clone[i][j])\n",
    "#             print(conv1_weights[i][j])\n",
    "            if elem != 0: # pomijamy sprunowane wagi\n",
    "                clone[i][j].fill_(conv1_weights[i][j].detach())\n",
    "    return clone                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92281314",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recoverOriginalWeightsW2():\n",
    "    clone = model.conv2.lin.weight.clone()\n",
    "    for i, neuron in enumerate(model.conv2.lin.weight):\n",
    "        for j, elem in enumerate(neuron) :\n",
    "            if elem != 0: # pomijamy sprunowane wagi\n",
    "                clone[i][j].fill_(conv2_weights[i][j].detach())\n",
    "    return clone                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46ad2150",
   "metadata": {},
   "outputs": [],
   "source": [
    "def printW(tensor):\n",
    "    counter = 0\n",
    "    for i, neuron in enumerate(tensor):\n",
    "        for j, elem in enumerate(neuron) :\n",
    "            if counter == 10:\n",
    "                break\n",
    "            print(tensor[i][j])\n",
    "            print(conv1_weights[i][j])\n",
    "            print(model.conv1.lin.weight[i][j])\n",
    "            counter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e48da89",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.conv1.lin.weight = recoverOriginalWeightsW1()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84e2bfdd",
   "metadata": {},
   "source": [
    "<font size=\"5\">Z jakiegoś powodu tutaj nie mogę zrobic pruningu na dwoch warstwach bo wywala kernel, musze sprawdzic osobno. Edit: osobno udało mi się tylko raz sprunowac pierwsza warstwe, ale to chyba bez sensu wrzucac do sprawka. Próbowałem też losowej reinicjalizacji wag, też wywala.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0f0d14e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def randomReinitalizeW1(original_tensor):\n",
    "    clone = original_tensor.clone()\n",
    "    for i, neuron in enumerate(original_tensor):\n",
    "        for j, elem in enumerate(neuron) :\n",
    "            if elem != 0: # pomijamy sprunowane wagi\n",
    "                tensor = torch.empty(1)\n",
    "                tensor.uniform_(-0.1, 0.1)\n",
    "                clone[i][j].fill_(tensor.item())\n",
    "    return clone                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f8cc0b32",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.conv1.lin.weight = randomReinitalizeW1(model.conv1.lin.weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a33252a",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = final_train(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3f44617",
   "metadata": {},
   "outputs": [],
   "source": [
    "test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9bad17f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters_to_prune = (\n",
    "    (model.conv1.lin, 'weight'),  # Prune the weights of layer1\n",
    "    (model.conv2.lin, 'weight'),  # Prune the weights of layer2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e83841e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "prune.global_unstructured(\n",
    "    parameters_to_prune,\n",
    "    pruning_method=prune.L1Unstructured,  # Use L1-norm based pruning\n",
    "    amount=0.4,  # Prune 20% of the parameters\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4acdaf04",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.conv1.lin.weight = recoverOriginalWeightsW1()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8caa271",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = final_train(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a94e1f57",
   "metadata": {},
   "outputs": [],
   "source": [
    "test()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26357ab3",
   "metadata": {},
   "source": [
    "<font size=\"5\">Dalsza część została wykonana w collabie ponieważ tam udało się zrobić pruning.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f2627bc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
